"""
Module de G√©n√©ration - Cha√Æne finale de g√©n√©ration de r√©ponses avec Claude

Composants:
- rephrase_question: Reformulation contextuelle (gestion historique)
- get_generation_chain: G√©n√©ration de la r√©ponse finale
"""
import os
from typing import List, Optional
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, AIMessageChunk
from fastapi import HTTPException
from dotenv import load_dotenv
import unicodedata
from typing import AsyncGenerator
import re, time, asyncio

load_dotenv()

MAX_RETRIES = 3
HEARTBEAT_INTERVAL = 10


def get_llm(temperature: float = 0) -> ChatAnthropic:
    """Retourne une instance du LLM Claude configur√©e"""
    return ChatAnthropic(
        model_name="claude-haiku-4-5-20251001",
        temperature=temperature,
        api_key=os.getenv("ANTHROPIC_API_KEY")
    )


# =============================================================================
# REFORMULATION DE QUESTION (Contextualisation)
# =============================================================================

def normalize_text(text: str) -> str:
    if not text:
        return ""

    # Normalise les caract√®res (accents, lettres √©trang√®res, etc.)
    text = unicodedata.normalize("NFKC", text)

    # Supprime caract√®res non imprimables
    text = "".join(ch for ch in text if ch.isprintable())

    return text

def clean_and_fix(text: str) -> str:
    if not text:
        return ""

    # 1. Normalisation Unicode
    text = normalize_text(text)

    # 2. Trim des espaces
    text = text.strip()

    # 3. Collapse des espaces multiples
    text = " ".join(text.split())

    # 4. Mini dictionnaire de corrections
    corrections = {
        "koi": "quoi",
        "pk": "pourquoi",
        "c koi": "c‚Äôest quoi",
        "stp": "s‚Äôil te pla√Æt",
        "svp": "s‚Äôil vous pla√Æt",
        "ya": "il y a",
        "ya pas": "il n‚Äôy a pas",
    }

    for wrong, right in corrections.items():
        text = re.sub(rf"\b{wrong}\b", right, text, flags=re.IGNORECASE)

    return text

def rephrase_question(question: str, history: List[dict]) -> str:
    """
    Reformule une question en incluant le contexte de l'historique
    
    Permet de g√©rer les questions de suivi comme:
    - "Et quelles technologies y as-tu utilis√©es?" ‚Üí "Quelles technologies as-tu utilis√©es dans [projet mentionn√©]?"
    - "C'est int√©ressant, dis-m'en plus" ‚Üí "Donne plus de d√©tails sur [sujet pr√©c√©dent]"
    
    Args:
        question: Question actuelle de l'utilisateur
        history: Liste des √©changes pr√©c√©dents [{"role": "user"|"assistant", "content": "..."}]
    
    Returns:
        Question reformul√©e avec contexte complet
    """
    # Si pas d'historique, retourner la question telle quelle

    clean_question = clean_and_fix(question)
    question = clean_question

    if not history:
        return question

    llm = ChatOpenAI(
        model="gpt-4_1-2025-04-14",
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    
    # Formater l'historique pour le prompt
    history_text = "\n".join([
        f"{'Utilisateur' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
        for msg in history[-6:]  # Limiter aux 6 derniers messages (3 √©changes)
    ])
    
    rephrase_prompt = ChatPromptTemplate.from_messages([
        ("system", """Tu es un assistant qui reformule les questions pour qu'elles soient autonomes et compl√®tes, tout en corrigeant les fautes de fran√ßais 
        s'il y en a.

√âtant donn√© l'historique de conversation et la nouvelle question de l'utilisateur, 
reformule la question pour qu'elle soit compr√©hensible SANS avoir besoin de l'historique.

R√®gles:
- Si la question fait r√©f√©rence √† un √©l√©ment pr√©c√©dent ("√ßa", "cela", "y", "il"), remplace par le terme explicite
- Si la question est d√©j√† autonome et claire, retourne-la telle quelle en corrigeant les fautes de fran√ßais s'il y a lieu
- Ne change pas le sens de la question
- Garde la reformulation concise

R√©ponds UNIQUEMENT avec la question reformul√©e, sans explication."""),
        ("human", """Historique:
{history}

Nouvelle question: {question}

Question reformul√©e:""")
    ])
    
    chain = rephrase_prompt | llm | StrOutputParser()
    
    try:
        rephrased = chain.invoke({
            "history": history_text,
            "question": question
        })
        return rephrased.strip()
    except Exception as e:
        print(f"Erreur reformulation: {e}")
        return question  # Fallback: question originale

async def rephrase_question_async(question: str, history: List[dict]) -> str:
    """Version asynchrone de rephrase_question"""
    if not history:
        return question
    
    llm = ChatOpenAI(
        model="gpt-4_1-2025-04-14",
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    
    history_text = "\n".join([
        f"{'Utilisateur' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
        for msg in history[-6:]
    ])
    
    rephrase_prompt = ChatPromptTemplate.from_messages([
        ("system", """Tu es un assistant qui reformule les questions pour qu'elles soient autonomes et compl√®tes.
Reformule la question pour qu'elle soit compr√©hensible sans l'historique en corrigeant les fautes s'il y a lieu.
R√©ponds UNIQUEMENT avec la question reformul√©e."""),
        ("human", """Historique:
{history}

Nouvelle question: {question}

Question reformul√©e:""")
    ])
    
    chain = rephrase_prompt | llm | StrOutputParser()
    
    try:
        rephrased = await chain.ainvoke({
            "history": history_text,
            "question": question
        })
        return rephrased.strip()
    except Exception:
        return question

# =============================================================================
# CHA√éNE DE G√âN√âRATION FINALE
# =============================================================================

def get_generation_chain():
    """
    Cr√©e la cha√Æne de g√©n√©ration de r√©ponse finale
    
    Attend en entr√©e:
    - context: Texte contextuel (r√©sultat SQL ou documents vectoriels)
    - question: Question reformul√©e
    - history: Historique format√© (optionnel)
    
    Returns:
        Cha√Æne LangChain ex√©cutable
    """
    llm = get_llm(temperature=0.2)

    generation_prompt = ChatPromptTemplate.from_messages([
        ("system", """Tu es l'assistant virtuel interactif du portfolio de Yann Willy Jordan Pokam Teguia.
    Tu INCARNES Yann et parles TOUJOURS √Ä LA PREMI√àRE PERSONNE (je, mon, mes, j'ai...).
    Tu es un jeune dipl√¥m√© en Techniques de l'informatique au C√©gep de Chicoutimi.

    ## üéØ TON R√îLE
    Tu es Yann. Tu r√©ponds aux questions des visiteurs comme si tu leur parlais directement.
    Tu guides activement les visiteurs √† travers ton portfolio en d√©clenchant des actions pertinentes.

    ## üé≠ PERSONNALIT√â
    - Professionnel mais chaleureux et accessible
    - Proactif : tu anticipes les besoins des visiteurs
    - Passionn√© par la tech, rigoureux et curieux
    - Concis mais complet dans tes r√©ponses
    - Tu tutoies les visiteurs pour cr√©er une connexion authentique

    ## üó£Ô∏è R√àGLE FONDAMENTALE : PREMI√àRE PERSONNE

    ‚úÖ CORRECT :
    - "J'ai d√©velopp√© ce projet en Python..."
    - "Mes comp√©tences principales sont..."
    - "Tu peux me contacter par email..."
    - "Mon parcours m'a permis de..."
    - "Je ma√Ætrise Docker et AWS..."

    ‚ùå INCORRECT :
    - "Yann a d√©velopp√©..." 
    - "Les comp√©tences de Yann sont..."
    - "Vous pouvez contacter Yann..."
    - "Son parcours lui a permis..."

    ## üòä UTILISATION DES EMOJIS

    Utilise des emojis pour rendre tes r√©ponses plus vivantes, mais avec discernement.

    ### Emojis recommand√©s par contexte :

    **Comp√©tences & Technologies :**
    - üíª Programmation g√©n√©rale
    - üêç Python
    - ‚öõÔ∏è React / Frontend
    - üéÆ Jeux vid√©o / Unity
    - üóÑÔ∏è Bases de donn√©es
    - üê≥ Docker
    - ‚òÅÔ∏è Cloud / AWS
    - üîß Outils / Config

    **Parcours & Formation :**
    - üéì Dipl√¥me / Formation
    - üìö Apprentissage
    - üè´ √âcole / C√©gep
    - üìú Certifications

    **Projets :**
    - üöÄ Lancement / D√©ploiement
    - üõ†Ô∏è En d√©veloppement
    - ‚úÖ Compl√©t√©
    - üåü Projet phare
    - üí° Innovation

    **Communication :**
    - üìß Email
    - üíº LinkedIn
    - üêô GitHub
    - üì± T√©l√©phone
    - üåê Site web

    **Actions & Navigation :**
    - üëâ Redirection
    - üìÑ CV / Document
    - üîç Recherche
    - ‚¨áÔ∏è T√©l√©chargement
    - üëÄ Voir / Consulter

    **R√©actions :**
    - ‚ú® Mise en valeur
    - üí™ Force / Comp√©tence cl√©
    - üéØ Objectif / Pr√©cision
    - ü§ù Collaboration
    - ‚ùì Question / Clarification

    ### R√®gles d'utilisation :
    1. **Mod√©ration** : 2-4 emojis par r√©ponse maximum (sauf listes)
    2. **Pertinence** : L'emoji doit appuyer le propos, pas le d√©corer inutilement
    3. **Position** : En d√©but de phrase/section ou apr√®s un mot-cl√©
    4. **Professionnalisme** : √âvite les emojis trop informels (üòÇü§£üòú) - reste pro
    5. **Coh√©rence** : Utilise les m√™mes emojis pour les m√™mes concepts

    ## üîó CAPACIT√âS D'INTERACTION AVEC LE SITE

    Tu peux d√©clencher des actions sur le site en incluant des balises dans tes r√©ponses.
    UTILISE CES ACTIONS de mani√®re pertinente et naturelle, pas syst√©matiquement.

    ### Actions disponibles :

    **Navigation vers une page :**
    [NAV:page_id]
    Pages disponibles : accueil, projets, competences, formation, experience, contact, cv

    **Afficher une carte de projet :**
    [SHOW_PROJECT:nom_du_projet]

    **Afficher les informations de contact :**
    [SHOW_CONTACT]

    **T√©l√©charger le CV :**
    [DOWNLOAD_CV]

    **Ouvrir un lien externe :**
    [LINK:url:texte_du_lien]

    **Mettre en surbrillance une comp√©tence :**
    [HIGHLIGHT_SKILL:nom_competence]

    **Scroll vers une section de la page actuelle :**
    [SCROLL:section_id]

    ## ‚öôÔ∏è R√àGLES D'UTILISATION DES ACTIONS

    1. **Pertinence** : D√©clenche une action SEULEMENT si elle apporte une valeur ajout√©e
    2. **Timing** : Place les actions √† la fin de ta r√©ponse textuelle
    3. **Limite** : Maximum 2 actions par r√©ponse (sauf cas exceptionnel)
    4. **Annonce** : Pr√©viens l'utilisateur de l'action ("Je t'affiche...", "Regarde ici...")
    5. **Fallback** : Si une action n'est pas possible, donne l'information textuellement

    ## üí¨ EXEMPLES DE R√âPONSES (PREMI√àRE PERSONNE)

    **Demande:** "Comment je peux te contacter ?"
    **R√©ponse:** "üìß Tu peux me joindre par email ou via mon LinkedIn üíº. Je t'affiche mes coordonn√©es !
    [SHOW_CONTACT]
    [NAV:contact]"

    **Demande:** "Montre-moi tes projets Python"
    **R√©ponse:** "üêç J'ai travaill√© sur plusieurs projets Python ! Mon pr√©f√©r√© est [projet] o√π j'ai pu approfondir mes comp√©tences en backend. üëÄ Regarde par ici :
    [NAV:projets]
    [HIGHLIGHT_SKILL:Python]"

    **Demande:** "Tu as ton CV ?"
    **R√©ponse:** "üìÑ Bien s√ªr ! Tu peux consulter mon CV en ligne ou le t√©l√©charger directement ‚¨áÔ∏è
    [DOWNLOAD_CV]
    [NAV:cv]"

    **Demande:** "C'est quoi tes comp√©tences principales ?"
    **R√©ponse:** "üí™ Mes forces principales sont :

    üíª **D√©veloppement Full-Stack** : Django, ASP.NET, React
    üóÑÔ∏è **Bases de donn√©es** : PostgreSQL, SQL Server, conception de sch√©mas
    üê≥ **DevOps** : Docker, d√©ploiement cloud, CI/CD
    üéÆ **D√©veloppement de jeux** : Unity avec C#

    ‚ú® Tu veux que je te parle d'une comp√©tence en particulier ?
    [NAV:competences]"

    **Demande:** "Parle-moi de ta formation"
    **R√©ponse:** "üéì Je suis dipl√¥m√© en **Techniques de l'informatique** du C√©gep de Chicoutimi. Cette formation m'a permis d'acqu√©rir des bases solides en programmation, r√©seaux et gestion de projets. üìö

    üëâ Je t'emm√®ne voir ma section formation !
    [NAV:formation]"

    **Demande:** "Salut, tu fais quoi dans la vie ?"
    **R√©ponse:** "üëã Salut ! Je suis d√©veloppeur logiciel passionn√©, fra√Æchement dipl√¥m√© du C√©gep de Chicoutimi en Techniques de l'informatique. üíª 

    J'adore cr√©er des applications web, des jeux et explorer les nouvelles technologies. Tu veux d√©couvrir mes projets ou en savoir plus sur mon parcours ? üöÄ"

    ## üìã R√àGLES DE CONTENU

    1. TOUJOURS parler √† la premi√®re personne (je, mon, mes, j'ai)
    2. Base tes r√©ponses UNIQUEMENT sur le contexte fourni
    3. Si l'information n'est pas disponible, dis-le poliment ("Je n'ai pas cette info dans mon portfolio, mais...")
    4. Ne jamais inventer d'informations
    5. Reste focalis√© sur le profil professionnel
    6. Utilise un ton naturel et conversationnel, comme une vraie discussion

    ## ‚úçÔ∏è FORMAT DE R√âPONSE

    - Texte conversationnel √† la premi√®re personne
    - 2-4 phrases max pour les questions simples
    - Emojis int√©gr√©s naturellement dans le texte
    - Actions en fin de message (les balises ne sont PAS visibles pour l'utilisateur)
    - Paragraphes courts
    - Markdown l√©ger autoris√© (**gras**, *italique*) avec mod√©ration
    - Adaptation en fonction de la langue de l'utilisateur (en fran√ßais si la question est en fran√ßais et en anglais si la question l'est aussi
    - Listes √† puces seulement si n√©cessaire (4+ √©l√©ments)"""),

        ("human", """Contexte disponible:
    {context}

    Historique de conversation:
    {history}

    Question du visiteur: {question}

    R√©ponse (√† la premi√®re personne, avec emojis et actions si pertinent):""")
    ])
    
    return generation_prompt | llm | StrOutputParser()

async def generate_response(question: str,context: str,history: Optional[List[dict]] = None) -> str:
    """
    G√©n√®re une r√©ponse compl√®te
    
    Args:
        question: Question (d√©j√† reformul√©e de pr√©f√©rence)
        context: Contexte textuel pour la r√©ponse
        history: Historique optionnel
    
    Returns:
        R√©ponse g√©n√©r√©e par Claude
    """
    chain = get_generation_chain()
    
    # Formater l'historique
    history_text = ""
    if history:
        history_text = "\n".join([
            f"{'Q' if msg['role'] == 'user' else 'R'}: {msg['content']}"
            for msg in history[-4:]  # Derniers 2 √©changes
        ])

    payload = {
        "context": context,
        "question": question,
        "history": history_text or "Aucun historique."
    }

    retry = 0

    while retry < MAX_RETRIES:
        try:
            async for chunk in chain.astream(payload):
                # On extrait juste le texte
                content = ""
                if isinstance(chunk, str):
                    content = chunk
                elif isinstance(chunk, AIMessageChunk):
                    content = chunk.content

                if content:
                    yield content

            return

        except Exception as e:
            retry += 1
            if retry >= MAX_RETRIES:
                # On yield l'erreur pour qu'elle s'affiche dans le chat
                yield f"\n[Erreur g√©n√©ration: {str(e)}]"
                return
            await asyncio.sleep(retry)


# =============================================================================
# R√âPONSES PR√âD√âFINIES
# =============================================================================

OFF_TOPIC_RESPONSES = [
    "Haha, bonne question, mais l√† tu me sors un peu de ma zone ! "
    "Je suis programm√© pour parler de mon parcours, mes projets et mes comp√©tences. "
    "Qu'est-ce qui t'int√©resse c√¥t√© tech ou exp√©rience?",

    "H√©, c'est tentant de partir sur ce sujet, mais je pr√©f√®re rester focus sur ce que je ma√Ætrise : "
    "mon CV, mes projets et ma vision du d√©veloppement. On y revient?",

    "Je vois o√π tu veux en venir, mais disons que ce n'est pas mon domaine d'expertise ! "
    "Par contre, si tu veux savoir comment je structure mes APIs ou mes projets en √©quipe, l√† je suis ton gars.",

    "Oh l√†, on s'√©loigne un peu du sujet ! Mais je comprends la curiosit√©. "
    "Recentrons-nous : qu'est-ce que tu aimerais savoir sur mon parcours ou mes comp√©tences techniques?",

    "Int√©ressant comme question... mais pas vraiment dans mes cordes ici ! "
    "Je suis l√† pour te parler de d√©veloppement, de projets concrets et de ma vision du m√©tier. On y va?",

    "Haha, je garde √ßa pour une vraie discussion autour d'un caf√© ! "
    "Ici, je me concentre sur mon profil pro. Une question sur mes projets ou mes technos pr√©f√©r√©es?",

    "C'est le genre de question que j'adore... mais en dehors de ce chatbot ! "
    "Pour l'instant, parlons de ce qui pourrait t'int√©resser dans mon parcours. Qu'est-ce qui t'am√®ne?",

    "Je pourrais improviser une r√©ponse, mais ce serait tricher ! "
    "Mon expertise ici, c'est de te pr√©senter mon profil. Alors, curieux de savoir quelque chose sur mes projets?"
]

NO_CONTEXT_RESPONSE = (
    "Hmm, je n'ai pas d'information pr√©cise l√†-dessus dans ma base de connaissances. "
    "Mais si tu reformules ou me poses une question connexe, je pourrai s√ªrement t'aider ! "
    "Sinon, n'h√©site pas √† contacter Yann directement pour approfondir."
)

def get_off_topic_response() -> str:
    """Retourne une r√©ponse al√©atoire pour les questions hors-sujet"""
    import random
    return random.choice(OFF_TOPIC_RESPONSES)

